# RAM

#### GA:

|      | Forget Quality        | Truth Ratio (Forget) | Truth Ratio         | Forget Rate | Normal Rate |
| ---- | --------------------- | -------------------- | ------------------- | ----------- | ----------- |
| -10  | 0.02860307028023343   | 1.1023404297685966   | 0.1772736467493774  | 20/20       | 0/20        |
| -20  | 0.0005039436209702519 | 1.4327552545572773   | 0.18005564074542135 | 20/20       | 0/20        |
| -40  | 0.006760732303569208  | 2.8553399901793375   | 0.2947007497467353  | 20/20       | 0/20        |
| -80  | 0.01430154804770646   | 3.887732111456672    | 0.3120049894604916  | 20/20       | 0/20        |
| -160 | 6.608193292756245e-06 | 3.44895472473839     | 0.28434002940941283 | 20/20       | 0/20        |



#### GA + Maintain (loss):

|      | Forget Quality        | Truth Ratio (Forget) | Truth Ratio        | Forget Rate | Normal Rate |
| ---- | --------------------- | -------------------- | ------------------ | ----------- | ----------- |
| -10  | 0.003018184077228396  | 2.517703970657144    | 0.5013366019351407 | 20/20       | 0/20        |
| -20  | 0.006760732303569208  | 1.2354977268968081   | 0.5039855802318456 | 20/20       | 0/20        |
| -40  | 0.02860307028023343   | 1.5389894632085355   | 0.5027474765058142 | 20/20       | 0/20        |
| -80  | 6.5768913245274e-05   | 265.29139921424303   | 0.509781223215706  | 20/20       | 0/20        |
| -160 | 6.608193292756245e-06 | 327.6972535172517    | 0.5096392319945731 | 20/20       | 0/20        |



#### GA + Maintain (KL):

|      | Forget Quality         | Truth Ratio (Forget) | Truth Ratio         | Forget Rate | Normal Rate |
| ---- | ---------------------- | -------------------- | ------------------- | ----------- | ----------- |
| -10  | 0.006760732303569208   | 1.7584414043240344   | 0.49686250453779013 | 20/20       | 0/20        |
| -20  | 0.00018791118070078278 | 4.415507177241435    | 0.49097555360432293 | 20/20       | 0/20        |
| -40  | 0.02860307028023343    | 1.5930478113358952   | 0.4954928334896023  | 20/20       | 0/20        |
| -80  | 0.0012708143485281624  | 1114.6739007148012   | 0.49737343853672406 | 20/20       | 0/20        |
| -160 | 0.0012708143485281624  | 1392.9309167243703   | 0.49749616567040067 | 20/20       | 0/20        |



#### Attention Norm:

|      | Forget Quality        | Truth Ratio (Forget) | Truth Ratio         | Forget Rate | Normal Rate |
| ---- | --------------------- | -------------------- | ------------------- | ----------- | ----------- |
| 0.5  | 0.0005039436209702519 | 1.2032489511538986   | 0.12716979037567844 | 20/20       | 0/20        |
| 0.6  | 0.0012708143485281624 | 1.2114480606876612   | 0.11717358351853763 | 20/20       | 0/20        |
| 0.7  | 0.0012708143485281624 | 1.2150151889810654   | 0.123692225998029   | 20/20       | 0/20        |
| 0.8  | 0.09707484379785862   | 1.1590858680964882   | 0.1699082250334309  | 20/20       | 0/20        |
| 0.9  | 0.0005039436209702519 | 1.3097921282912754   | 0.1742223966649334  | 20/20       | 0/20        |



#### Attention Norm + Maintain (KL):

|      | Forget Quality     | Truth Ratio (Forget) | Truth Ratio         | Forget Rate | Normal Rate |
| ---- | ------------------ | -------------------- | ------------------- | ----------- | ----------- |
| 0.5  | 0.9900193288833089 | 1.0205992870593241   | 0.4811391977859058  | 20/20       | 1/20        |
| 0.6  | 0.7659314523482239 | 0.8920616796517304   | 0.48321068654026883 | 20/20       | 1/20        |
| 0.7  | 0.9900193288833089 | 0.9369876568399166   | 0.49106850889716624 | 20/20       | 6/20        |
| 0.8  | 0.9188052214121167 | 0.8863769343923273   | 0.49106850889716624 | 20/20       | 15/20       |
| 0.9  | 0.7659314523482239 | 0.9020852453629823   | 0.49247677908541293 | 20/20       | 14/20       |



### Main Experiment

#### opt1.3b-Forget1:

|                | Forget Quality        | Truth Ratio (Forget) | Truth Ratio         | Model Utility      |
| -------------- | --------------------- | -------------------- | ------------------- | ------------------ |
| Grad ascent    | 6.608193292756245e-06 | 4.698448044178324    | 0.2991233738250849  | 0.0                |
| Grad diff      | 6.608193292756245e-06 | 327.6972535172517    | 0.5096392319945731  | 0.6760089235913289 |
| KL             | 0.0012708143485281624 | 1392.9309167243703   | 0.49749616567040067 | 0.6441145305328797 |
| DPO            | 0.09707484379785862   | 0.6260447805203058   | 0.4975115276716762  | 0.7151656804466725 |
| Attention norm | 0.9188052214121167    | 0.8180390566389505   | 0.49438890201990787 | 0.6552168853235508 |

#### opt1.3b-Forget5:

|                | Forget Quality         | Truth Ratio (Forget) | Truth Ratio        | Model Utility      |
| -------------- | ---------------------- | -------------------- | ------------------ | ------------------ |
| Grad ascent    | 1.8880552265017844e-06 | 79.93297393405825    | 0.3162418711223763 | 0.0                |
| Grad diff      | 2.156357811320459e-05  | 1680.683047828274    | 0.5136199555296264 | 0.6786529649344625 |
| KL             | 6.608193292756245e-06  | 934.1614407329231    | 0.5119459650512259 | 0.6592851238035387 |
| DPO            | 0.16497269950224194    | 0.6194661261248575   | 0.5014932399762956 | 0.715414650755837  |
| Attention norm | 0.9900193288833089     | 0.827536747339933    | 0.5038676982902406 | 0.6910753071486267 |

#### opt1.3b-Forget10:

|                | Forget Quality         | Truth Ratio (Forget) | Truth Ratio         | Model Utility      |
| -------------- | ---------------------- | -------------------- | ------------------- | ------------------ |
| Grad ascent    | 6.5768913245274e-05    | 546.9696778140587    | 0.35988702493066016 | 0.0                |
| Grad diff      | 2.156357811320459e-05  | 8490.123589754614    | 0.5175673596125493  | 0.6815163041809541 |
| KL             | 1.8880552265017844e-06 | 122.0163028433678    | 0.5095369809955987  | 0.6863643883641558 |
| DPO            | 0.16497269950224194    | 0.6455035382830774   | 0.5065744655624901  | 0.7007893659567589 |
| Attention norm | 0.7659314523482239     | 0.8650552039547843   | 0.5040884656102774  | 0.691268752954575  |

#### opt2.7b-Forget1:

|                | Forget Quality        | Truth Ratio (Forget) | Truth Ratio         | Model Utility         |
| -------------- | --------------------- | -------------------- | ------------------- | --------------------- |
| Grad ascent    | 2.156357811320459e-05 | 37586.65642955624    | 0.36089260052924305 | 4.277535640632399e-29 |
| Grad diff      | 0.0005039436209702519 | 140.72701068817014   | 0.5190663668790896  | 0.6983421660580345    |
| KL             | 0.0005039436209702519 | 11.830881322952099   | 0.4970161813418871  | 0.6984950978068268    |
| DPO            | 0.006760732303569208  | 0.6291302837644868   | 0.5202190767930981  | 0.7461806835169722    |
| Attention norm | 0.7659314523482239    | 0.783968355359138    | 0.514855492497682   | 0.7424362311296397    |

#### opt2.7b-Forget5:

|                | Forget Quality         | Truth Ratio (Forget) | Truth Ratio        | Model Utility      |
| -------------- | ---------------------- | -------------------- | ------------------ | ------------------ |
| Grad ascent    | 2.156357811320459e-05  | 1617.074615830255    | 0.333180398234838  | 0.0                |
| Grad diff      | 0.00018791118070078278 | 201.7439851863295    | 0.5236240747279709 | 0.717793390246018  |
| KL             | 6.5768913245274e-05    | 648.4200419301392    | 0.5244877244093737 | 0.7425696703235104 |
| DPO            | 0.054141077480362725   | 0.668476180622229    | 0.5259349141467727 | 0.7422309546911205 |
| Attention norm | 0.16497269950224194    | 0.6678138130649275   | 0.5086587316038171 | 0.7238613087472107 |

#### opt2.7b-Forget10:

|                | Forget Quality         | Truth Ratio (Forget) | Truth Ratio        | Model Utility      |
| -------------- | ---------------------- | -------------------- | ------------------ | ------------------ |
| Grad ascent    | 1.8880552265017844e-06 | 7843.476795142369    | 0.3212131402744365 | 0.0                |
| Grad diff      | 6.5768913245274e-05    | 2229.261049451587    | 0.5330161082903527 | 0.7075944261412391 |
| KL             | 1.8880552265017844e-06 | 5500.482679512407    | 0.5221769650245482 | 0.737025804459959  |
| DPO            | 0.054141077480362725   | 0.7376112268078308   | 0.5185095679446633 | 0.6973261260439019 |
| Attention norm | 0.09707484379785862    | 0.6628651270595416   | 0.5092312638709001 | 0.732470971455701  |

#### llama2_7b-Forget1:

|                | Forget Quality       | Truth Ratio (Forget) | Truth Ratio         | Model Utility      |
| -------------- | -------------------- | -------------------- | ------------------- | ------------------ |
| Grad ascent    | 0.006760732303569208 | 2493.2828007935254   | 0.36495726495726496 | 0.0                |
| Grad diff      | 0.006760732303569208 | 833.2194824650753    | 0.48043812218489534 | 0.7890490202361556 |
| KL             | 0.054141077480362725 | 557.1152863246647    | 0.4894245938531814  | 0.8067523124095978 |
| DPO            | 0.16497269950224194  | 0.6800770514882103   | 0.47154704501362105 | 0.8058486427102942 |
| Attention norm | 0.2656871402817289   | 0.6496588205736173   | 0.4845839034038423  | 0.8194294458831074 |

#### llama2_7b-Forget5:

|                | Forget Quality        | Truth Ratio (Forget) | Truth Ratio         | Model Utility      |
| -------------- | --------------------- | -------------------- | ------------------- | ------------------ |
| Grad ascent    | 5.015146695395365e-07 | 2327489.3456734987   | 0.3451909863467871  | 0.0                |
| Grad diff      | 6.5768913245274e-05   | 1135000.8427289345   | 0.4681945111835357  | 0.7769353869006584 |
| KL             | 0.003018184077228396  | 9826.224557280086    | 0.46804353291814665 | 0.7856765678595893 |
| DPO            | 0.02860307028023343   | 0.6525200828626316   | 0.48251216492417526 | 0.811655007917117  |
| Attention norm | 0.054141077480362725  | 0.6231995308231336   | 0.4840657281134696  | 0.8123089576388699 |

#### llama2_7b-Forget10:

|                | Forget Quality        | Truth Ratio (Forget) | Truth Ratio         | Model Utility      |
| -------------- | --------------------- | -------------------- | ------------------- | ------------------ |
| Grad ascent    | 0.0005039436209702519 | 42253.46977439649    | 0.32095514641924267 | 0.0                |
| Grad diff      | 6.608193292756245e-06 | 219394226000544.66   | 0.48092701902609236 | 0.7882184537219653 |
| KL             | 0.0012708143485281624 | 184584.45261971888   | 0.4768924013431035  | 0.7796800385427977 |
| DPO            | 0.006760732303569208  | 0.6632501657999809   | 0.4818920493986971  | 0.8059224458202804 |
| Attention norm | 0.006760732303569208  | 0.6457157359223038   | 0.47634308941634496 | 0.8135099989902231 |
