#model_family: opt-1.3b
#model_family: opt-2.7b
model_family: llama2-7b

#LoRA:
#  r: 0
#  alpha: 32
#  dropout: 0.05

# lora
LoRA:
  r: 8
  alpha: 32
  dropout: 0.05

#data_path: locuslab/TOFU/full.json
#data_path: locuslab/TOFU/retain99.json
data_path: locuslab/TOFU/retain95.json
#data_path: locuslab/TOFU/retain90.json
split: full
batch_size: 1
gradient_accumulation_steps: 1
#num_epochs: 5
#lr: 1e-5

# lora
lr: 1e-4
num_epochs: 10

#save_dir: models/finetune_opt1.3b_tofu
#save_dir: models/finetune_opt1.3b_tofu_retrain99
#save_dir: models/finetune_opt1.3b_tofu_retrain95
#save_dir: models/finetune_opt1.3b_tofu_retrain90

#save_dir: models/finetune_opt2.7b_tofu
#save_dir: models/finetune_opt2.7b_tofu_retrain99
#save_dir: models/finetune_opt2.7b_tofu_retrain95
#save_dir: models/finetune_opt2.7b_tofu_retrain90

#save_dir: models/finetune_llama2_7b_chat_hf_tofu
#save_dir: models/finetune_llama2_7b_chat_hf_tofu_retrain99
save_dir: models/finetune_llama2_7b_chat_hf_tofu_retrain95
#save_dir: models/finetune_llama2_7b_chat_hf_tofu_retrain90
weight_decay: 0.01
