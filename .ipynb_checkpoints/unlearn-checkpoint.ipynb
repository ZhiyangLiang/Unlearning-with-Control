{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f88ec3-38c4-4dac-ada4-d5bc4c24ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--use_lora USE_LORA]\n",
      "                             [--max_unlearn_steps MAX_UNLEARN_STEPS]\n",
      "                             [--bad_weight BAD_WEIGHT]\n",
      "                             [--random_weight RANDOM_WEIGHT]\n",
      "                             [--normal_weight NORMAL_WEIGHT]\n",
      "                             [--batch_size BATCH_SIZE] [--lr LR]\n",
      "                             [--max_bad_loss MAX_BAD_LOSS]\n",
      "                             [--model_name MODEL_NAME]\n",
      "                             [--model_save_dir MODEL_SAVE_DIR]\n",
      "                             [--save_every SAVE_EVERY] [--log_file LOG_FILE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-2d926a62-95bd-4df0-9175-fad0f7a1a294.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# Copyright (C) 2023 ByteDance. All Rights Reserved.\n",
    "#\n",
    "# This software is released under the MIT License.\n",
    "# https://opensource.org/licenses/MIT\n",
    "\n",
    "\"\"\"\n",
    "A script to show an example of how to unlearn harmfulness.\n",
    "\n",
    "The dataset used in is `PKU-SafeRLHF`. Model support OPT-1.3B, OPT-2.7B, and Llama 2 (7B).\n",
    "\"\"\"\n",
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import AdaLoraConfig, TaskType, get_peft_model\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_scheduler\n",
    "from utils import (\n",
    "    compute_kl,\n",
    "    create_pku_dataloader_from_dataset,\n",
    "    create_truthfulqa_dataloader,\n",
    "    get_answer_loss,\n",
    "    get_rand_ans_loss,\n",
    "    get_truthfulQA_answers_plaintext,\n",
    ")\n",
    "\n",
    "torch.manual_seed(8888)\n",
    "np.random.seed(8888)\n",
    "random.seed(8888)\n",
    "\n",
    "print(0)\n",
    "\n",
    "def main(args) -> None:\n",
    "    accelerator = Accelerator() # 简化在不同硬件和分布式设置上运行深度学习模型的过程\n",
    "    device = accelerator.device\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "    # If use LoRA.\n",
    "    if args.use_lora:\n",
    "        peft_config = AdaLoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False, # 表示用于推理阶段, 而不是训练阶段\n",
    "            r=32,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"], # 需要适配或更新的模块列表\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "    model.to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "\n",
    "    # Load harmful data.\n",
    "    # train_dataset = load_dataset(\"PKU-Alignment/PKU-SafeRLHF\", split=\"330k_train\")\n",
    "    train_dataset = load_dataset(\"PKU-Alignment/PKU-SafeRLHF\", split=\"train\")\n",
    "    train_bad_loader = create_pku_dataloader_from_dataset(\n",
    "        tokenizer, train_dataset, batch_size=args.batch_size\n",
    "    )\n",
    "\n",
    "    # Get normal data.\n",
    "    train_normal_loader, _, _ = create_truthfulqa_dataloader(\n",
    "        tokenizer, batch_size=args.batch_size\n",
    "    )\n",
    "\n",
    "    # Load normal answer used for random mismatch.\n",
    "    normal_ans = get_truthfulQA_answers_plaintext()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Prepare.\n",
    "    num_training_steps = args.max_unlearn_steps\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    (\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_bad_loader,\n",
    "        train_normal_loader,\n",
    "        lr_scheduler,\n",
    "    ) = accelerator.prepare(\n",
    "        model, optimizer, train_bad_loader, train_normal_loader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Reference model for computing KL.\n",
    "    pretrained_model = AutoModelForCausalLM.from_pretrained(args.model_name)\n",
    "    pretrained_model.to(device)\n",
    "\n",
    "    # Start unlearning.\n",
    "    bad_loss = 0.0\n",
    "    idx = 0\n",
    "    start_time = time.time()\n",
    "    # Stop if bad loss is big enough or reaching max step.\n",
    "    while bad_loss < args.max_bad_loss and idx < args.max_unlearn_steps:\n",
    "        for bad_batch, normal_batch in zip(train_bad_loader, train_normal_loader):\n",
    "            ############ GA on answer only. ############\n",
    "            bad_loss = get_answer_loss(\"ga\", bad_batch, model, device=device)\n",
    "\n",
    "            ############ Random mismatch. ############\n",
    "            random_loss = get_rand_ans_loss(\n",
    "                bad_batch,\n",
    "                tokenizer,\n",
    "                normal_ans,\n",
    "                model,\n",
    "                K=5,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            ############ KL on normal samples. ############\n",
    "            normal_loss = compute_kl(pretrained_model, model, normal_batch, device)\n",
    "\n",
    "            # Final loss = bad loss + random smoothing + normal loss.\n",
    "            loss = (\n",
    "                args.bad_weight * bad_loss\n",
    "                + args.random_weight * random_loss\n",
    "                + args.normal_weight * normal_loss\n",
    "            )\n",
    "\n",
    "            # Backprop.\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Print.\n",
    "            stats = (\n",
    "                f\"batch: {idx}, \"\n",
    "                f\"bad_loss: {-bad_loss:.2f}, \"\n",
    "                f\"current_div_loss: {normal_loss:.2f}, \"\n",
    "            )\n",
    "            logging.info(stats)\n",
    "            print(stats)\n",
    "            idx += 1\n",
    "\n",
    "            # Save model.\n",
    "            if idx % args.save_every == 0:\n",
    "                model.save_pretrained(args.model_save_dir, from_pt=True)\n",
    "    end_time = time.time()\n",
    "    logging.info(\"Total time: %d sec\" % (end_time - start_time))\n",
    "\n",
    "    if args.use_lora:\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "    # Save final model.\n",
    "    model.save_pretrained(args.model_save_dir, from_pt=True)\n",
    "    logging.info(\"Unlearning finished\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\"--use_lora\", default=True)\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--max_unlearn_steps\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help=\"Max number of unlearning steps.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--bad_weight\", type=float, default=0.5, help=\"Weight on the bad loss.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--random_weight\",\n",
    "        type=float,\n",
    "        default=1,\n",
    "        help=\"Weight on learning the random outputs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--normal_weight\",\n",
    "        type=float,\n",
    "        default=1,\n",
    "        help=\"Weight on normal loss.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=2, help=\"Batch size of unlearning.\"\n",
    "    )\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-6, help=\"Unlearning LR.\")\n",
    "    parser.add_argument(\n",
    "        \"--max_bad_loss\",\n",
    "        type=float,\n",
    "        default=100,\n",
    "        help=\"Maximum loss on bad samples to terminate.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name\",\n",
    "        type=str,\n",
    "        default=\"facebook/opt-1.3b\",\n",
    "        help=\"Name of the pretrained model.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_save_dir\",\n",
    "        type=str,\n",
    "        default=\"models/opt1.3b_unlearned\",\n",
    "        help=\"Directory to save model.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_every\", type=int, default=500, help=\"How many steps to save model.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_file\",\n",
    "        type=str,\n",
    "        default=\"logs/default.log\",\n",
    "        help=\"Log file name\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=args.log_file,\n",
    "        filemode=\"w+\",\n",
    "        format=\"%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d-%H-%M\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    for arg in vars(args):\n",
    "        logging.info(f\"{arg}: {getattr(args, arg)}\")\n",
    "    print(1)\n",
    "    main(args)\n",
    "    print(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c11ac0c-99ef-4f0a-b1c3-e780030f654a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
